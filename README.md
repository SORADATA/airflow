# ğŸŒªï¸ Projet Airflow : Weather ETL Orchestration

Pipeline de donnÃ©es automatisÃ© pour l'extraction, la transformation et le chargement de donnÃ©es mÃ©tÃ©orologiques, orchestrÃ© via Apache Airflow et dÃ©ployÃ© avec la CLI Astronomer.

---

## ğŸ“‹ Table des matiÃ¨res

- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Structure du projet](#-structure-du-projet)
- [Configuration du DAG](#ï¸-configuration-du-dag)
- [Utilisation](#-utilisation)
- [DÃ©veloppement](#-dÃ©veloppement)
- [Notes importantes](#-notes-importantes)

---

## ğŸ”§ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© :

- **Python 3.8+** : Environnement de dÃ©veloppement
- **Docker Desktop** : Pour l'exÃ©cution des conteneurs Airflow (Webserver, Scheduler, Database)
- **Astro CLI** : Interface en ligne de commande pour gÃ©rer l'environnement Airflow

---

## ğŸš€ Installation

### 1. Installation de la CLI Astronomer

**Linux/macOS :**
```bash
curl -sSL install.astronomer.io | sudo bash -s
```

**Windows :**  
Consultez la [documentation officielle](https://www.astronomer.io/docs/astro/cli/install-cli?tab=windows)

### 2. Initialisation du projet

```bash
# Cloner le dÃ©pÃ´t
git clone <votre-repo>
cd <nom-du-projet>

# Initialiser l'environnement Airflow (si pas dÃ©jÃ  fait)
astro dev init

# DÃ©marrer l'environnement
astro dev start
```

### 3. AccÃ¨s Ã  l'interface

Une fois dÃ©marrÃ©, accÃ©dez Ã  l'interface Airflow : **http://localhost:8080**

**Identifiants par dÃ©faut :**
- Username : `admin`
- Password : `admin`

---

## ğŸ“ Structure du projet

```
.
â”œâ”€â”€ dags/                    # DAGs Python (cÅ“ur du pipeline)
â”‚   â””â”€â”€ weather_etl.py       # DAG principal pour les donnÃ©es mÃ©tÃ©o
â”œâ”€â”€ include/                 # Fichiers auxiliaires (SQL, scripts)
â”œâ”€â”€ plugins/                 # Hooks et opÃ©rateurs personnalisÃ©s
â”œâ”€â”€ tests/                   # Tests unitaires pour validation
â”œâ”€â”€ Dockerfile               # Image Docker personnalisÃ©e
â”œâ”€â”€ packages.txt             # DÃ©pendances systÃ¨me (OS-level)
â”œâ”€â”€ requirements.txt         # BibliothÃ¨ques Python
â””â”€â”€ airflow_settings.yaml    # Configuration Airflow personnalisÃ©e
```

### Description des rÃ©pertoires

- **dags/** : Contient tous vos workflows Airflow
- **include/** : Scripts SQL, fichiers de configuration, utilitaires
- **plugins/** : Extensions Airflow personnalisÃ©es
- **tests/** : Tests unitaires et d'intÃ©gration
- **requirements.txt** : SpÃ©cifiez ici vos dÃ©pendances Python (ex: `requests`, `pandas`, `sqlalchemy`)

---

## âš™ï¸ Configuration du DAG

Le DAG `weather_etl` est configurÃ© pour une exÃ©cution quotidienne avec les paramÃ¨tres suivants :

```python
from airflow import DAG
from datetime import datetime

with DAG(
    dag_id="weather_etl",
    start_date=datetime(2024, 1, 1, 9, 0),
    schedule="@daily",                        # ExÃ©cution quotidienne Ã  9h
    catchup=True,                             # Rattrapage des dates passÃ©es
    max_active_runs=1,                        # Une seule instance simultanÃ©e
    render_template_as_native_obj=True,       # Rendu natif des templates Jinja
    tags=["weather", "ETL", "production"],
) as dag:
    # DÃ©finition des tÃ¢ches ci-dessous
    pass
```

### ParamÃ¨tres clÃ©s

| ParamÃ¨tre | Valeur | Description |
|-----------|--------|-------------|
| `schedule` | `@daily` | ExÃ©cution automatique chaque jour |
| `catchup` | `True` | Rattrape les exÃ©cutions manquÃ©es depuis `start_date` |
| `max_active_runs` | `1` | EmpÃªche les conflits en limitant les exÃ©cutions parallÃ¨les |

---

## ğŸ¯ Utilisation

### Commandes essentielles

| Commande | Description |
|----------|-------------|
| `astro dev start` | DÃ©marre l'environnement Airflow local |
| `astro dev stop` | ArrÃªte tous les conteneurs Docker |
| `astro dev restart` | RedÃ©marre l'environnement (nÃ©cessaire aprÃ¨s modification de `requirements.txt`) |
| `astro dev ps` | Affiche l'Ã©tat des services Airflow |
| `astro dev logs` | Consulte les logs en temps rÃ©el |
| `astro dev bash` | Ouvre un shell dans le conteneur webserver |

### Workflow typique

```bash
# 1. Modifier votre DAG
vim dags/weather_etl.py

# 2. RedÃ©marrer pour appliquer les changements
astro dev restart

# 3. VÃ©rifier les logs
astro dev logs -f scheduler
```

---

## ğŸ› ï¸ DÃ©veloppement

### Ajouter une dÃ©pendance Python

1. Ã‰ditez `requirements.txt` :
```txt
requests==2.31.0
pandas==2.1.0
```

2. RedÃ©marrez l'environnement :
```bash
astro dev restart
```

### Ajouter un package systÃ¨me

Ã‰ditez `packages.txt` pour installer des dÃ©pendances OS (ex: bibliothÃ¨ques systÃ¨me) :
```txt
build-essential
libpq-dev
```

### Tester votre DAG localement

```bash
# Tester l'import du DAG
astro dev bash
python -c "from dags.weather_etl import dag"

# Tester une tÃ¢che spÃ©cifique
airflow tasks test weather_etl ma_tache 2024-01-01
```

---

## ğŸ“ Notes importantes

### Backfilling

GrÃ¢ce Ã  `catchup=True`, Airflow exÃ©cutera automatiquement toutes les dates manquÃ©es entre `start_date` et aujourd'hui. Pour dÃ©sactiver ce comportement :

```python
catchup=False  # Ne rattrape pas les exÃ©cutions passÃ©es
```

### Variables et connexions

Configurez vos variables et connexions via l'interface Airflow ou le fichier `airflow_settings.yaml` :

```yaml
airflow:
  connections:
    - conn_id: weather_api
      conn_type: http
      host: https://api.weather.com
  variables:
    - variable_name: api_key
      variable_value: your_secret_key
```

### Bonnes pratiques

- Utilisez des **Variables** Airflow pour les configurations sensibles
- Activez `max_active_runs=1` pour les DAGs manipulant des ressources partagÃ©es
- Testez toujours vos DAGs localement avant le dÃ©ploiement
- Documentez vos tÃ¢ches avec des `doc_md` pour faciliter la maintenance

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour proposer des amÃ©liorations :

1. Forkez le projet
2. CrÃ©ez une branche (`git checkout -b feature/amelioration`)
3. Committez vos changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. Pushez vers la branche (`git push origin feature/amelioration`)
5. Ouvrez une Pull Request

---

## ğŸ“š Ressources

- [Documentation Apache Airflow](https://airflow.apache.org/docs/)
- [Documentation Astronomer](https://www.astronomer.io/docs/)
- [Tutoriels Airflow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)

---

**DÃ©veloppÃ© avec â˜ï¸ par [SISSOKO Moussa]**